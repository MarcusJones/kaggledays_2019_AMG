{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Jun 10 10:32:09 2018\n",
    "\n",
    "@author: m.jones\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/MarcusJones/kaggle_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "==========================================================================="
   },
   "outputs": [],
   "source": [
    "# Logging\n",
    "# =============================================================================\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "#Delete Jupyter notebook root logger handler\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "\n",
    "# Set level\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create formatter\n",
    "#FORMAT = \"%(asctime)s - %(levelno)-3s - %(module)-10s  %(funcName)-10s: %(message)s\"\n",
    "#FORMAT = \"%(asctime)s - %(levelno)-3s - %(funcName)-10s: %(message)s\"\n",
    "#FORMAT = \"%(asctime)s - %(funcName)-10s: %(message)s\"\n",
    "FORMAT = \"%(asctime)s : %(message)s\"\n",
    "DATE_FMT = \"%Y-%m-%d %H:%M:%S\"\n",
    "#DATE_FMT = \"%H:%M:%S\"\n",
    "formatter = logging.Formatter(FORMAT, DATE_FMT)\n",
    "\n",
    "# Create handler and assign\n",
    "handler = logging.StreamHandler(sys.stderr)\n",
    "handler.setFormatter(formatter)\n",
    "logger.handlers = [handler]\n",
    "logging.info(\"Logging started\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Globals"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# LANDSCAPE_A3 = (16.53, 11.69)\n",
    "# PORTRAIT_A3 = (11.69, 16.53)\n",
    "# LANDSCAPE_A4 = (11.69, 8.27)\n",
    "if 'KAGGLE_WORKING_DIR' in os.environ:\n",
    "    DEPLOYMENT = 'Kaggle'\n",
    "else:\n",
    "    DEPLOYMENT = 'Local'\n",
    "logging.info(\"Deployment: {}\".format(DEPLOYMENT))\n",
    "if DEPLOYMENT=='Kaggle':\n",
    "    # PATH_DATA_ROOT = Path.cwd() / '..' / 'input'\n",
    "    SAMPLE_FRACTION = 1\n",
    "    # import transformers as trf\n",
    "if DEPLOYMENT == 'Local':\n",
    "    # PATH_DATA_ROOT = r\"~/DATA/petfinder_adoption\"\n",
    "    SAMPLE_FRACTION = 1\n",
    "    # import kaggle_utils.transformers as trf\n",
    "\n",
    "\n",
    "# PATH_OUT = r\"/home/batman/git/hack_sfpd1/Out\"\n",
    "# PATH_OUT_KDE = r\"/home/batman/git/hack_sfpd1/out_kde\"\n",
    "# PATH_REPORTING = r\"/home/batman/git/hack_sfpd1/Reporting\"\n",
    "# PATH_MODELS = r\"/home/batman/git/hack_sfpd4/models\"\n",
    "# TITLE_FONT = {'fontname': 'helvetica'}\n",
    "\n",
    "\n",
    "# TITLE_FONT_NAME = \"Arial\"\n",
    "# plt.rc('font', family='Helvetica')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "==========================================================================="
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "# =============================================================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import time\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "==========================================================================="
   },
   "outputs": [],
   "source": [
    "# ML imports\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "print('numpy', np.__version__)\n",
    "import pandas as pd\n",
    "print('pandas', pd.__version__)\n",
    "import sklearn as sk\n",
    "print('sklearn', sk.__version__)\n",
    "\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import sklearn.linear_model\n",
    "import sklearn.pipeline\n",
    "import sklearn.model_selection\n",
    "\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "# Models\n",
    "import lightgbm as lgb\n",
    "print(\"lightgbm\", lgb.__version__)\n",
    "import xgboost as xgb\n",
    "print(\"xgboost\", xgb.__version__)\n",
    "from catboost import CatBoostClassifier\n",
    "import catboost as catb\n",
    "print(\"catboost\", catb.__version__)\n",
    "\n",
    "# Metric\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "def kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "==========================================================================="
   },
   "outputs": [],
   "source": [
    "# Custom imports\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from fancyimpute import SimpleFill\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def rmsle(real, predicted):\n",
    "   sum=0.0\n",
    "   for x in range(len(predicted)):\n",
    "       if predicted[x]<0 or real[x]<0: #check for negative values\n",
    "           continue\n",
    "       p = np.log(predicted[x]+1)\n",
    "       r = np.log(real[x]+1)\n",
    "       sum = sum + (p - r)**2\n",
    "   return (sum/len(predicted))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeit(method):\n",
    "    \"\"\" Decorator to time execution of transformers\n",
    "    :param method:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        if 'log_time' in kw:\n",
    "            name = kw.get('log_name', method.__name__.upper())\n",
    "            kw['log_time'][name] = int((te - ts) * 1000)\n",
    "        else:\n",
    "            print(\"\\t {} {:2.1f}s\".format(method.__name__, (te - ts)))\n",
    "        return result\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "class PandasSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None, dtype=None, inverse=False,\n",
    "                 return_vector=True, name=None):\n",
    "        self.dtype = dtype\n",
    "        self.columns = columns\n",
    "        self.inverse = inverse\n",
    "        self.return_vector = return_vector\n",
    "        self.name = name\n",
    "\n",
    "        if isinstance(self.columns, str):\n",
    "            self.columns = [self.columns]\n",
    "\n",
    "        logging.info(\"Init {} on cols: {}\".format(name, columns))\n",
    "\n",
    "    def check_condition(self, x, col):\n",
    "        cond = (self.dtype is not None and x[col].dtype == self.dtype) or \\\n",
    "               (self.columns is not None and col in self.columns)\n",
    "        return self.inverse ^ cond\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def _check_if_all_columns_present(self, x):\n",
    "        if not self.inverse and self.columns is not None:\n",
    "            missing_columns = set(self.columns) - set(x.columns)\n",
    "            if len(missing_columns) > 0:\n",
    "                missing_columns_ = ','.join(col for col in missing_columns)\n",
    "                raise KeyError(\"Keys are missing in the record: {}, columns required:{}\".format( missing_columns_, self.columns))\n",
    "\n",
    "    def transform(self, x):\n",
    "        logging.info(\"{} is transforming...\".format(self.name))\n",
    "        # check if x is a pandas DataFrame\n",
    "        if not isinstance(x, pd.DataFrame):\n",
    "            raise KeyError(\"Input is not a pandas DataFrame it's a {}\".format(type(x)))\n",
    "\n",
    "        selected_cols = []\n",
    "        for col in x.columns:\n",
    "            if self.check_condition(x, col):\n",
    "                selected_cols.append(col)\n",
    "\n",
    "        # if the column was selected and inversed = False make sure the column\n",
    "        # is in the DataFrame\n",
    "        self._check_if_all_columns_present(x)\n",
    "\n",
    "        # if only 1 column is returned return a vector instead of a dataframe\n",
    "        if len(selected_cols) == 1 and self.return_vector:\n",
    "            return list(x[selected_cols[0]])\n",
    "        else:\n",
    "            return x[selected_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLog():\n",
    "    \"\"\"Add a .log attribute for logging\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def log(self):\n",
    "        return \"Transformer: {}\".format(type(self).__name__)\n",
    "\n",
    "# %%==============================================================================\n",
    "# Imputer1D - Simple Imputer wrapper\n",
    "# ===============================================================================\n",
    "class Imputer1D(sk.preprocessing.Imputer):\n",
    "    \"\"\"\n",
    "    A simple wrapper class on Imputer to avoid having to make a single column 2D.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        if X.ndim == 1:\n",
    "            X = np.expand_dims(X, axis=1)\n",
    "        # Call the Imputer as normal, return result\n",
    "        return super(Imputer1D, self).fit(X, y=None)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if X.ndim == 1:\n",
    "            X = np.expand_dims(X, axis=1)\n",
    "            # Call the Imputer as normal, return result\n",
    "        return super(Imputer1D, self).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class MultipleToNewFeature(sk.base.BaseEstimator, sk.base.TransformerMixin, TransformerLog):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, selected_cols, new_col_name,func):\n",
    "        self.selected_cols = selected_cols\n",
    "        self.new_col_name = new_col_name\n",
    "        self.func = func\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    @timeit\n",
    "    def transform(self, df, y=None):\n",
    "        # print(df)\n",
    "        df[self.new_col_name] = df.apply(self.func, axis=1)\n",
    "        print(self.log, \"{}({}) -> ['{}']\".format(self.func.__name__,self.selected_cols,self.new_col_name))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalToCat(sk.base.BaseEstimator, sk.base.TransformerMixin):\n",
    "    \"\"\"Convert numeric indexed column into dtype category with labels\n",
    "    Convert a column which has a category, presented as an Integer\n",
    "    Initialize with a dict of ALL mappings for this session, keyed by column name\n",
    "    (This could be easily refactored to have only the required mapping)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, label_map_dict, allow_more_labels=False):\n",
    "        self.label_map_dict = label_map_dict\n",
    "        self.allow_more_labels = allow_more_labels\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def get_unique_values(self, this_series):\n",
    "        return list(this_series.value_counts().index)\n",
    "\n",
    "    def transform(self, this_series):\n",
    "        if not self.allow_more_labels:\n",
    "            if len(self.label_map_dict) > len(this_series.value_counts()):\n",
    "                msg = \"{} labels provided, but {} values in column!\\nLabels:{}\\nValues:{}\".format(\n",
    "                    len(self.label_map_dict), len(this_series.value_counts()), self.label_map_dict,\n",
    "                    self.get_unique_values(this_series), )\n",
    "                raise ValueError(msg)\n",
    "\n",
    "        if len(self.label_map_dict) < len(this_series.value_counts()):\n",
    "            raise ValueError\n",
    "\n",
    "        assert type(this_series) == pd.Series\n",
    "        # assert this_series.name in self.label_map_dict, \"{} not in label map!\".format(this_series.name)\n",
    "        return_series = this_series.copy()\n",
    "        # return_series = pd.Series(pd.Categorical.from_codes(this_series, self.label_map_dict))\n",
    "        return_series = return_series.astype('category')\n",
    "        return_series.cat.rename_categories(self.label_map_dict, inplace=True)\n",
    "        # print(return_series.cat.categories)\n",
    "\n",
    "        assert return_series.dtype == 'category'\n",
    "        return return_series\n",
    "\n",
    "#%%==============================================================================\n",
    "# WordCounter\n",
    "# ===============================================================================\n",
    "class WordCounter(sk.base.BaseEstimator, sk.base.TransformerMixin, TransformerLog):\n",
    "    \"\"\" Count the words in the input column\n",
    "    \"\"\"\n",
    "    def __init__(self, col_name, new_col_name):\n",
    "        self.col_name = col_name\n",
    "        self.new_col_name = new_col_name\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        new_col = df[self.col_name].apply(lambda x: len(x.split(\" \")))\n",
    "        df[self.new_col_name] = new_col\n",
    "        print(self.log, self.new_col_name)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "============================================================================="
   },
   "outputs": [],
   "source": [
    "# ConvertToDatetime\n",
    "# ===============================================================================\n",
    "class ConvertToDatetime(sk.base.BaseEstimator, sk.base.TransformerMixin, TransformerLog):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, time_col_name, unit='s'):\n",
    "        self.time_col_name = time_col_name\n",
    "        self.unit = unit\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        df[self.time_col_name] = pd.to_datetime(df[self.time_col_name], unit=self.unit)\n",
    "        print(\"Transformer:\", type(self).__name__, \"converted\", self.time_col_name, \"to dt\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "============================================================================="
   },
   "outputs": [],
   "source": [
    "# TimeProperty\n",
    "# ===============================================================================\n",
    "class TimeProperty(sk.base.BaseEstimator, sk.base.TransformerMixin, TransformerLog):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, time_col_name, new_col_name, time_property):\n",
    "        \"\"\"\n",
    "\n",
    "        :param time_col_name: Source column, MUST BE A datetime TYPE!\n",
    "        :param new_col_name: New column name\n",
    "        :param time_property: hour, month, dayofweek\n",
    "        \"\"\"\n",
    "        self.time_col_name = time_col_name\n",
    "        self.new_col_name = new_col_name\n",
    "        self.time_property = time_property\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        original_shape = df.shape\n",
    "        if self.time_property == 'hour':\n",
    "            df[self.new_col_name] = df[self.time_col_name].dt.hour\n",
    "        elif self.time_property == 'month':\n",
    "            df[self.new_col_name] = df[self.time_col_name].dt.month\n",
    "        elif self.time_property == 'dayofweek':\n",
    "            df[self.new_col_name] = df[self.time_col_name].dayofweek\n",
    "        else:\n",
    "            raise\n",
    "        print(\"Transformer:\", type(self).__name__, original_shape, \"->\", df.shape, vars(self))\n",
    "        return df\n",
    "# Debug:\n",
    "# df = X_train\n",
    "# time_col_name = 'question_utc'\n",
    "# new_col_name = 'question_hour'\n",
    "# time_property = 'hour'\n",
    "# time_col_name = 'question_utc'\n",
    "# new_col_name = 'question_month'\n",
    "# time_property = 'month'\n",
    "# time_adder = TimeProperty(time_col_name,new_col_name,time_property)\n",
    "# res=time_adder.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "============================================================================="
   },
   "outputs": [],
   "source": [
    "# DEPRECIATED - AnswerDelay\n",
    "# ===============================================================================\n",
    "class AnswerDelay(sk.base.BaseEstimator, sk.base.TransformerMixin, TransformerLog):\n",
    "    \"\"\" Used once, not general, gets time elapsed\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, new_col_name, divisor=1):\n",
    "        self.new_col_name = new_col_name\n",
    "        self.divisor = divisor\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        df[self.new_col_name] = df['answer_utc'] - df['question_utc']\n",
    "        df[self.new_col_name] = df[self.new_col_name].dt.seconds / self.divisor\n",
    "        print(self.log)\n",
    "        return df\n",
    "\n",
    "\n",
    "# Debug:\n",
    "# df = X_train\n",
    "# new_col_name = 'answer_delay_seconds'\n",
    "# answer_delay_adder = AnswerDelay(new_col_name)\n",
    "# res=answer_delay_adder.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "============================================================================="
   },
   "outputs": [],
   "source": [
    "# ValueCounter\n",
    "# ===============================================================================\n",
    "class ValueCounter(sk.base.BaseEstimator, sk.base.TransformerMixin, TransformerLog):\n",
    "    \"\"\"??\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, col_name):\n",
    "        self.col_name = col_name\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        # Count the number of unique entries in a column\n",
    "        # reset_index() is used to maintain the DataFrame for merging\n",
    "        selected_df_col = df[self.col_name].value_counts().reset_index()\n",
    "        # Create a new name for this column\n",
    "        selected_df_col.columns = [self.col_name, self.col_name + '_counts']\n",
    "        print(self.log)\n",
    "        return pd.merge(selected_df_col, df, on=self.col_name)\n",
    "\n",
    "# %%=============================================================================\n",
    "# DEPRECIATED ConvertDoubleColToDatetime\n",
    "# ===============================================================================\n",
    "class ConvertDoubleColToDatetime(sk.base.BaseEstimator, sk.base.TransformerMixin, TransformerLog):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    # pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    def __init__(self, new_col_name, name_col1, name_col2, this_format):\n",
    "        self.new_col_name = new_col_name\n",
    "        self.name_col1 = name_col1\n",
    "        self.name_col2 = name_col2\n",
    "        self.this_format = this_format\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    @timeit\n",
    "    def transform(self, df, y=None):\n",
    "        combined_date_string_series = df.loc[:, self.name_col1] + \" \" + df.loc[:, self.name_col2]\n",
    "        with ChainedAssignment():\n",
    "            df.loc[:, self.new_col_name] = pd.to_datetime(combined_date_string_series, format=self.this_format)\n",
    "        #        pd.options.mode.chained_assignment = 'warn'  # default='warn'\n",
    "\n",
    "        # print(\"Transformer:\", type(self).__name__, \"converted\", self.new_col_name, \"to dt\")\n",
    "        print(self.log)\n",
    "        return df\n",
    "\n",
    "# Debug:\n",
    "# df = sfpd_head\n",
    "# new_col_name = 'dt'\n",
    "# time_adder = ConvertDoubleColToDatetime(new_col_name,name_col1=\"Date\", name_col2=\"Time\",this_format=r'%m/%d/%Y %H:%M')\n",
    "# res=time_adder.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "DEBUG TRF"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# class TransformerLog():\n",
    "#     \"\"\"Add a .log attribute for logging\n",
    "#     \"\"\"\n",
    "#     @property\n",
    "#     def log(self):\n",
    "#         return \"Transformer: {}\".format(type(self).__name__)\n",
    "# class NumericalToCat(sk.base.BaseEstimator, sk.base.TransformerMixin, TransformerLog):\n",
    "#     \"\"\"Convert numeric indexed column into dtype category with labels\n",
    "#     Convert a column which has a category, presented as an Integer\n",
    "#     Initialize with a dict of ALL mappings for this session, keyed by column name\n",
    "#     (This could be easily refactored to have only the required mapping)\n",
    "#     \"\"\"\n",
    "#     def __init__(self,label_map):\n",
    "#         self.label_map = label_map\n",
    "#\n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "#\n",
    "#     def transform(self, this_series):\n",
    "#         assert type(this_series) == pd.Series\n",
    "#         mapped_labels = list(self.label_map.values())\n",
    "#         # assert this_series.name in self.label_map_dict, \"{} not in label map!\".format(this_series.Name)\n",
    "#         return_series = this_series.copy()\n",
    "#         return_series = pd.Series(pd.Categorical.from_codes(this_series, mapped_labels))\n",
    "#         # return_series = return_series.astype('category')\n",
    "#         # return_series.cat.rename_categories(self.label_map_dict[return_series.name], inplace=True)\n",
    "#         print(self.log, mapped_labels, return_series.cat.categories, )\n",
    "#         assert return_series.dtype == 'category'\n",
    "#         return return_series\n",
    "#\n",
    "# # this_series = df_all['Vaccinated'].copy()\n",
    "# # this_series.value_counts()\n",
    "# # label_map = label_maps['Vaccinated']\n",
    "# # mapped_labels = list(label_map.values())\n",
    "# # my_labels = pd.Index(mapped_labels)\n",
    "# # pd.Series(pd.Categorical.from_codes(this_series, my_labels))\n",
    "#\n",
    "# for col_name in label_maps:\n",
    "#     df_all[col_name].value_counts().index\n",
    "#     print(col_name)\n",
    "#     label_maps[col_name]\n",
    "#     df_all.replace({col_name: label_maps[col_name]},inplace=True)\n",
    "#\n",
    "#\n",
    "#\n",
    "# df_all['Vaccinated'] = df_all['Vaccinated'] - 1\n",
    "#\n",
    "# pandas.CategoricalIndex.reorder_categories\n",
    "#\n",
    "# # To return the original integer mapping!\n",
    "# ivd = {v: k for k, v in label_maps['State'].items()}\n",
    "# df_all['State'].astype('object').replace(ivd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "==========================================================================="
   },
   "outputs": [],
   "source": [
    "# Data source and paths\n",
    "# =============================================================================\n",
    "# path_data = Path(PATH_DATA_ROOT, r\"\").expanduser()\n",
    "# assert path_data.exists(), \"Data path does not exist: {}\".format(path_data)\n",
    "# logging.info(\"Data path {}\".format(PATH_DATA_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "==========================================================================="
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "# =============================================================================\n",
    "logging.info(f\"Loading files into memory\")\n",
    "#product data\n",
    "df_train = pd.read_csv('../input/train.csv')\n",
    "df_test = pd.read_csv('../input/test.csv')\n",
    "\n",
    "logging.info(\"Loaded train {}\".format(df_train.shape))\n",
    "logging.info(\"Loaded test {}\".format(df_test.shape))\n",
    "\n",
    "# Add a column to label the source of the data\n",
    "df_train['dataset_type'] = 'train'\n",
    "df_test['dataset_type'] = 'test'\n",
    "\n",
    "logging.info(\"Added dataset_type column for origin\".format())\n",
    "\n",
    "# Set this aside for debugging\n",
    "#TODO: Remove later\n",
    "original_y_train = df_train['target'].copy()\n",
    "\n",
    "df_all = pd.concat([df_train, df_test], sort=False)\n",
    "index_col = 'ID'\n",
    "df_all.set_index(index_col, inplace=True)\n",
    "\n",
    "logging.info(\"Concatenated dataset on {}, with origin column 'dataset_type', shape: {}\".format(index_col, df_all.shape))\n",
    "del df_train, df_test, index_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "OTHER DATA"
   },
   "outputs": [],
   "source": [
    "\n",
    "#sales, exchange rates, social network data\n",
    "df_sales = pd.read_csv('../input/sales.csv')\n",
    "logging.info(\"df_sales {}\".format(df_sales.shape))\n",
    "series_sales_null_count = df_sales.isnull().sum(axis=0)\n",
    "# r = series_sales_null_count[0]\n",
    "for row in [row for row in series_sales_null_count.iteritems() if row[1]]:\n",
    "    print(row, row[1]/df_sales.shape[0])\n",
    "\n",
    "# df_sales.describe()\n",
    "# df_sales.info()\n",
    "# sales_counts = df_sales.apply(pd.value_counts(dropna=False))\n",
    "\n",
    "#website navigation data\n",
    "df_navigation = pd.read_csv('../input/navigation.csv')\n",
    "logging.info(\"df_navigation {}\".format(df_navigation.shape))\n",
    "\n",
    "#product images vectorized with ResNet50\n",
    "df_vimages = pd.read_csv('../input/vimages.csv')\n",
    "logging.info(\"df_vimages {}\".format(df_vimages.shape))\n",
    "logging.info(\"Loaded other data \".format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "IMPUTATION"
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Impute missing sales\n",
    "sales_float_columns = df_sales.dtypes[df_sales.dtypes == 'float64'].index.tolist()\n",
    "df_sales.loc[:, sales_float_columns] = SimpleFill(fill_method='random').fit_transform(df_sales.loc[:, sales_float_columns])\n",
    "logging.info(\"Sales columns filled\".format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "navigation_null_count = df_navigation.isnull().sum(axis=0)\n",
    "# r = series_sales_null_count[0]\n",
    "for row in [row for row in navigation_null_count.iteritems() if row[1]]:\n",
    "    print(row, row[1]/df_sales.shape[0])\n",
    "\n",
    "# Impute missing website_version_zone_number\n",
    "df_navigation.loc[df_navigation.website_version_zone_number.isna(), 'website_version_zone_number'] = 'unknown'\n",
    "# Impute missing website_version_country_number\n",
    "df_navigation.loc[df_navigation.website_version_country_number.isna(), 'website_version_country_number'] = 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_null_count = df_all.color.isnull().sum(axis=0)\n",
    "# Impute missing colors\n",
    "df_all.loc[df_all.color.isna(), 'color'] = 'unknown'\n",
    "logging.info(\"Filled {} colors\".format(color_null_count))\n",
    "\n",
    "import re\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def extract_number_from_sent_strg(x):\n",
    "    number_find = re.compile(r'.*(\\d).*')\n",
    "    return int(number_find.match(str(x)).group(1))\n",
    "\n",
    "def return_lr_coeff(y):\n",
    "\n",
    "    # Reshape data\n",
    "    X = np.arange(1,8).reshape(-1,1)\n",
    "    y = y.values.reshape(-1,1)\n",
    "\n",
    "    # Fit regression model\n",
    "    l_model = LinearRegression(n_jobs=-1)\n",
    "    l_model.fit(X,y)\n",
    "\n",
    "    return l_model.coef_[0][0]\n",
    "\n",
    "\n",
    "def extract_sentiment_lin_reg_coefficients(enriched_sales_df, sentiment):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "       Extract linear regression coefficients (slope and intercept)\n",
    "       from the enriched_sales_df.\n",
    "       Sentiment is a string that specifies which values to use. You should use\n",
    "       one of the following:\n",
    "           - BuzzPost\n",
    "           - Buzz_\n",
    "           - NetSent\n",
    "           - Positive\n",
    "           - Negative\n",
    "           - Impressions\n",
    "\n",
    "       Args:\n",
    "           enriched_sales_df: pandas DataFrame\n",
    "           sentiment: string\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract sentiment columns\n",
    "    sentiment_compiler = re.compile(r'.*{}.*before'.format(sentiment))\n",
    "    sentiment_columns = [c for c in enriched_sales_df if sentiment_compiler.match(c)]\n",
    "\n",
    "    # Simple check\n",
    "    if sentiment_columns == []:\n",
    "        print(\"Wrong sentiment string\")\n",
    "        raise ValueError()\n",
    "\n",
    "    # Group by product\n",
    "    time_series_df = enriched_sales_df.groupby(['sku_hash'])[sentiment_columns].mean()\n",
    "\n",
    "    # Melt\n",
    "    time_series_df = time_series_df.reset_index().melt(id_vars=['sku_hash'], value_vars = sentiment_columns)\n",
    "\n",
    "    # Extract number from sent string\n",
    "    time_series_df.loc[:, 'days_before'] = (time_series_df.variable.transform(extract_number_from_sent_strg) * -1)+8\n",
    "\n",
    "    # Drop variable column\n",
    "    time_series_df.drop(columns=['variable'], inplace=True)\n",
    "\n",
    "    # Pivot back\n",
    "    time_series_df = time_series_df.pivot(index='sku_hash', columns='days_before', values='value')\n",
    "\n",
    "    # Backward fill NA\n",
    "    time_series_df.fillna(axis=1, method='backfill', inplace=True)\n",
    "\n",
    "    # Apply and calculate regression coefficient\n",
    "    reg_coeff_df = time_series_df.apply(return_lr_coeff, axis=1).reset_index()\n",
    "\n",
    "    # Rescale beta\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    reg_coeff_df.iloc[:,1] = scaler.fit_transform(reg_coeff_df.iloc[:,1].values.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "    # Rename columns\n",
    "    reg_coeff_df.columns = ['sku_hash', sentiment + '_beta']\n",
    "\n",
    "    return reg_coeff_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Currency and social columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales data breakdown:\n",
    "#\n",
    "# Currency:\n",
    "# 5 Currency rates: USD, GBP, CNY, JPY, KRW x 8 days = 40\n",
    "#\n",
    "# Social:\n",
    "# TotalBuzzPost\n",
    "# TotalBuzz\n",
    "# PositiveSentiment\n",
    "# NegativeSentiment\n",
    "# NetSentiment\n",
    "# Impressions\n",
    "# TOTAL = 6\n",
    "# And for each of the 7 days = 42\n",
    "# 40 + 48 = 88 columns\n",
    "def log_df(df,name):\n",
    "    logging.info(\"DataFrame {} {}\".format(name,df.shape))\n",
    "log_df(df_all,'df_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "INJECT NEW SALES TIME TREND FEATURE"
   },
   "outputs": [],
   "source": [
    "\n",
    "trend_columns = ['BuzzPost', 'Buzz_', 'NetSent', 'Positive', 'Negative', 'Impressions']\n",
    "\n",
    "for feature_col in trend_columns:\n",
    "    extracted = extract_sentiment_lin_reg_coefficients(df_sales,feature_col)\n",
    "    df_sales = df_sales.merge(extracted,on='sku_hash')\n",
    "    logging.info(\"Appended {}\".format(feature_col))\n",
    "    log_df(df_sales, 'df_sales')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"SALES DATA\".format())\n",
    "\n",
    "cols_currency_and_social = df_sales.columns[9:].tolist()\n",
    "print(cols_currency_and_social)\n",
    "logging.info(\"{} currency_and_social_columns columns\".format(len(cols_currency_and_social)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "This is a dataframe of ONLY the first day of activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first_day = df_sales.loc[df_sales.Date == 'Day_1', :]\n",
    "log_df(df_first_day,'df_first_day')\n",
    "df_second_day = df_sales.loc[df_sales.Date == 'Day_2', :]\n",
    "log_df(df_first_day,'df_second_day')\n",
    "# print(first_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean of ALL columns for ALL sku\n",
    "df_all_currency_and_social = df_sales.groupby('sku_hash').mean()[cols_currency_and_social]\n",
    "#\n",
    "df_first_day_currency_and_social = df_first_day.groupby('sku_hash').mean()[cols_currency_and_social]\n",
    "df_first_day_currency_and_social.columns = ['first_day_' + col for col in df_first_day_currency_and_social.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_sales = df_sales.groupby('sku_hash').sum()['sales_quantity']\n",
    "df_total_sales = pd.DataFrame(df_total_sales)\n",
    "df_first_day_sales = df_first_day.groupby(['sku_hash', 'day_transaction_date', 'Month_transaction']).sum()['sales_quantity']\n",
    "df_first_day_sales = pd.DataFrame(df_first_day_sales)\n",
    "df_first_day_sales.columns = ['first_day_sales']\n",
    "df_first_day_sales.reset_index(inplace=True)\n",
    "df_first_day_sales.set_index('sku_hash', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge total with first day\n",
    "df_sales_data = pd.merge(df_total_sales, df_first_day_sales, left_index=True, right_index=True)\n",
    "logging.info(\"Merged df_total_sales {} df_first_day_sales {} = {}\".format(df_total_sales.shape, df_first_day_sales.shape, df_sales_data.shape))\n",
    "\n",
    "# Merge all currency/social\n",
    "df_sales_data = pd.merge(df_sales_data, df_all_currency_and_social, left_index=True, right_index=True)\n",
    "logging.info(\"Merged df_all_currency_and_social {} = {}\".format(df_sales_data.shape, df_sales_data.shape))\n",
    "\n",
    "# Merge first day curr/soc\n",
    "df_sales_data = pd.merge(df_sales_data, df_first_day_currency_and_social, left_index=True, right_index=True)\n",
    "logging.info(\"Merged df_first_day_currency_and_social {} {} = {}\".format(df_first_day_currency_and_social.shape, df_first_day_sales.shape, df_sales_data.shape))\n",
    "log_df(df_sales_data,'df_sales_data')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthDict = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', 7:'Jul', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
    "df_sales_data.Month_transaction = df_sales_data.Month_transaction.astype('object').map(monthDict)\n",
    "logging.info(\"Mapped months in df_sales_data\".format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "logging.info(\"NAVIGATION DATA\".format())\n",
    "df_first_day_navigation = df_navigation.loc[df_navigation.Date == 'Day 1', :]\n",
    "df_first_day_views = df_first_day_navigation.groupby('sku_hash').sum()[['page_views', 'addtocart']]\n",
    "df_first_day_views.columns = ['first_day_page_views', 'first_day_addtocart']\n",
    "df_views = df_navigation.groupby('sku_hash').sum()[['page_views', 'addtocart']]\n",
    "df_navigation_data = pd.merge(df_views, df_first_day_views, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to float\n",
    "df_sales_data.sales_quantity = df_sales_data.sales_quantity.astype('float64')\n",
    "df_sales_data.first_day_sales = df_sales_data.first_day_sales.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Log transform the sales\n",
    "df_sales_data['sales_quantity_log'] = (df_sales_data.sales_quantity + 1).apply(np.log)\n",
    "df_sales_data['first_day_sales_log'] = (df_sales_data.first_day_sales + 1).apply(np.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Merge to main table!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_all_merged = pd.merge(df_all, df_sales_data, left_on='sku_hash', right_index=True)\n",
    "df_all_merged = pd.merge(df_all_merged, df_navigation_data, how='left', left_on='sku_hash', right_index=True)\n",
    "df_all_merged = pd.merge(df_all_merged, df_vimages, left_on='sku_hash', right_on='sku_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_merged[df_navigation_data.columns] = df_all_merged[df_navigation_data.columns].fillna(0)\n",
    "log_df(df_all_merged,'df_all_merged')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "107def94e3025396616fa08411e8260d6e7b4fb6",
    "lines_to_next_cell": 0
   },
   "source": [
    "## separate models for each prediction month\n",
    "df_all_merged = df_all_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate\n",
    "\n",
    "dfs_monthly_list = dict()\n",
    "for i in range(1, 4):\n",
    "    dfs_monthly_list[i] = dict()\n",
    "    dfs_monthly_list[i]['df'] = df_all_merged.loc[df_all_merged.month == i, :].copy()\n",
    "    log_df(dfs_monthly_list[i]['df'], 'Month 1 DF, indexed by ID')\n",
    "    # dfs_monthly_list[i].set_index('ID', inplace=True)\n",
    "    # logging.info(\"Set index ID\".format())\n",
    "    dfs_monthly_list[i]['df'].drop(['month', 'sku_hash'], axis=1, inplace=True)\n",
    "    logging.info(\"Dropped month, sku_hash\".format())\n",
    "    assert 'month' not in dfs_monthly_list[i]['df'].columns\n",
    "    assert 'sku_hash' not in dfs_monthly_list[i]['df'].columns\n",
    "    assert 'ID' not in dfs_monthly_list[i]['df'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT\n",
    "for i in range(1, 4):\n",
    "    df = dfs_monthly_list[i]['df']\n",
    "\n",
    "    # Training df\n",
    "    dfs_monthly_list[i]['df_tr'] = df[df['dataset_type'] == 'train'].copy()\n",
    "    dfs_monthly_list[i]['df_tr'].drop('dataset_type', axis=1, inplace=True)\n",
    "    log_df(dfs_monthly_list[i]['df_tr'], 'df_tr ' + str(i))\n",
    "\n",
    "    # X_tr\n",
    "    dfs_monthly_list[i]['X_tr'] = dfs_monthly_list[i]['df_tr'].drop('target', axis=1)\n",
    "    log_df(dfs_monthly_list[i]['X_tr'], 'X_tr ' + str(i))\n",
    "\n",
    "    # y_tr\n",
    "    dfs_monthly_list[i]['y_tr'] = pd.DataFrame(dfs_monthly_list[i]['df_tr']['target'])\n",
    "    log_df(dfs_monthly_list[i]['y_tr'] , 'y_tr ' + str(i))\n",
    "\n",
    "    logging.info(\"Applying log transform\".format())\n",
    "    dfs_monthly_list[i]['y_tr']['target'] = (dfs_monthly_list[i]['y_tr']['target'] + 1).apply(np.log)\n",
    "\n",
    "    # Test df\n",
    "    dfs_monthly_list[i]['df_te'] = df[df['dataset_type'] == 'test'].copy()\n",
    "    dfs_monthly_list[i]['df_te'].drop('dataset_type', axis=1, inplace=True)\n",
    "    log_df(dfs_monthly_list[i]['df_te'], 'df_te ' + str(i))\n",
    "\n",
    "    # X_te\n",
    "    dfs_monthly_list[i]['X_te'] = dfs_monthly_list[i]['df_te'].drop('target', axis=1)\n",
    "    log_df(dfs_monthly_list[i]['X_te'], 'X_te ' + str(i))\n",
    "\n",
    "\n",
    "a = dfs_monthly_list[i]['y_tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_cols = list()\n",
    "images_cols = df_vimages.columns[1:].tolist()\n",
    "float_cols = dfs_monthly_list[1]['df'].dtypes[dfs_monthly_list[1]['df'].dtypes == 'float64'].index.tolist()\n",
    "logging.info(\"{} float cols\".format(len(float_cols)))\n",
    "float_cols = list(set(float_cols) - set(images_cols))\n",
    "logging.info(\"{} float cols\".format(len(float_cols)))\n",
    "float_cols.remove('sales_quantity_log')\n",
    "float_cols.remove('first_day_sales_log')\n",
    "float_cols.remove('sales_quantity')\n",
    "float_cols.remove('first_day_sales')\n",
    "logging.info(\"{} float cols\".format(len(float_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "categorical_cols = list()\n",
    "logging.info(\"{} categorical cols\".format(len(float_cols)))\n",
    "categorical_cols = dfs_monthly_list[1]['df'].dtypes[dfs_monthly_list[1]['df'].dtypes == 'object'].index.tolist()\n",
    "categorical_cols.remove('en_US_description')\n",
    "categorical_cols.remove('color')\n",
    "logging.info(\"{} categorical cols\".format(len(float_cols)))\n",
    "dfs_monthly_list = dfs_monthly_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "if 'target' in float_cols: float_cols.remove('target')\n",
    "if 'dataset_type' in float_cols: float_cols.remove('dataset_type')\n",
    "assert 'target' not in float_cols\n",
    "assert 'dataset_type' not in float_cols\n",
    "\n",
    "\n",
    "if 'target' in categorical_cols: categorical_cols.remove('target')\n",
    "if 'dataset_type' in categorical_cols: categorical_cols.remove('dataset_type')\n",
    "assert 'target' not in categorical_cols\n",
    "assert 'dataset_type' not in categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "66bd00938a631cb806de3f1ade45ddd25a0119ec",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "my_sales_cols =['sales_quantity_log', 'first_day_sales_log', 'sales_quantity', 'first_day_sales']\n",
    "pipeline = make_pipeline(\n",
    "    make_union(\n",
    "        make_pipeline(\n",
    "            PandasSelector(columns='en_US_description',name='Description'),\n",
    "            CountVectorizer(stop_words='english'),\n",
    "            LatentDirichletAllocation(n_components=10)),\n",
    "        make_pipeline(\n",
    "            PandasSelector(columns='color',name='color'),\n",
    "            CountVectorizer() ),\n",
    "        make_pipeline(\n",
    "            PandasSelector(columns=images_cols,name='Images_cols'),\n",
    "            PCA(10)),\n",
    "        make_pipeline(\n",
    "            PandasSelector(columns=float_cols, name='Floats'),\n",
    "            PCA(10)),\n",
    "        make_pipeline(\n",
    "            PandasSelector(columns=my_sales_cols,name='Sales stuff')),\n",
    "        make_pipeline(\n",
    "            PandasSelector(columns=categorical_cols, name ='Categoricals'),\n",
    "            OneHotEncoder(handle_unknown='ignore'),\n",
    "            LatentDirichletAllocation(n_components=10))\n",
    "    ),\n",
    "    SelectFromModel(RandomForestRegressor(n_estimators=100)),\n",
    "    DecisionTreeRegressor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {'decisiontreeregressor__min_samples_split': [40, 60, 80],\n",
    "          'decisiontreeregressor__max_depth': [4, 6, 8]}\n",
    "\n",
    "\n",
    "grid_search_list = dict()\n",
    "\n",
    "for i in range(1,4):\n",
    "    logging.info('Month {}'.format(i))\n",
    "    grid_search_list[i] = None\n",
    "\n",
    "    grid_search_list[i] = GridSearchCV(pipeline, param_grid=params, cv=4, verbose=3, n_jobs=-1)\n",
    "\n",
    "    grid_search_list[i].fit(dfs_monthly_list[i]['X_tr'], dfs_monthly_list[i]['y_tr'])\n",
    "\n",
    "    dfs_monthly_list[i]['y_te'] = grid_search_list[i].predict(dfs_monthly_list[i]['X_te'])\n",
    "\n",
    "    dfs_monthly_list[i]['y_tr_pred'] = grid_search_list[i].predict(dfs_monthly_list[i]['X_tr'])\n",
    "\n",
    "    this_y_tr_pred = pd.Series(dfs_monthly_list[i]['y_tr_pred'])\n",
    "    this_y_tr = dfs_monthly_list[i]['y_tr']\n",
    "    this_y_tr = this_y_tr.iloc[:,0]\n",
    "    # compare = pd.DataFrame([dfs_monthly_list[i]['y_tr'],dfs_monthly_list[i]['y_tr']])\n",
    "    dfs_monthly_list[i]['compare'] = pd.DataFrame.from_records(\n",
    "        {'y_tr': this_y_tr,\n",
    "         'y_tr_pred': this_y_tr_pred}\n",
    "    ).reset_index()\n",
    "    logging.info('metric cv: {}'.format( np.round(np.sqrt(grid_search_list[i].best_score_), 4)))\n",
    "    logging.info('metric train: {}'.format(np.round(np.sqrt(mean_squared_error(this_y_tr, this_y_tr_pred)), 4)))\n",
    "    logging.info('params: {}'.format(grid_search_list[i].best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "SUBMISSION"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4d9d4dc8361c4dc285d2283bd58cd7465a0b0e61",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "for i in range(1,4):\n",
    "    logging.info('Month {}'.format(i))\n",
    "    grid_search_list[i]\n",
    "    dfs_monthly_list[i]['y_submit'] = (pd.Series(dfs_monthly_list[i]['y_te'])).apply(np.exp)  - 1\n",
    "    a = dfs_monthly_list[i]['df']\n",
    "\n",
    "    dfs_monthly_list[i]['y_submit'].index = dfs_monthly_list[i]['X_te'].index\n",
    "\n",
    "submission = pd.DataFrame(pd.concat([dfs_monthly_list[1]['y_submit'],\n",
    "                                     dfs_monthly_list[2]['y_submit'],\n",
    "                                     dfs_monthly_list[3]['y_submit']]))\n",
    "\n",
    "submission.index = df_all[df_all['dataset_type'] == 'test'].copy().index\n",
    "submission.columns = ['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e886a8013dcef7729789dffdbe5bb32270934c5b",
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "submission.describe()\n",
    "submission.to_csv('submission.csv', index_label='ID')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
